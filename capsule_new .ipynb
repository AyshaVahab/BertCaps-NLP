{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"V100","mount_file_id":"1gU8ACmEDvrbPprHhKmJMRh9w_PdHn8y3","authorship_tag":"ABX9TyM8yxx8CXSg1WpDr7ED7WsI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"MmBsrgyhCjOC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","# Load the data from the CSV file\n","data = pd.read_csv(\"/content/drive/MyDrive/new/data.csv\")\n","\n","# Split the data into train and test sets\n","train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n","\n","# Save the train and test sets as separate CSV files\n","train_data.to_csv(\"train.csv\", index=False)\n","test_data.to_csv(\"test.csv\", index=False)"],"metadata":{"id":"9HJ2qrZ_Paeg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install transformers\n"],"metadata":{"id":"Qc-GU4eRQUB4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Load data from CSV file\n","data = pd.read_csv('/content/drive/MyDrive/new/data.csv')\n","\n","# Extract text and labels from data\n","data.head()\n"],"metadata":{"id":"q8F9-GinSgO6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Extract text and labels from data\n","text = data['tweet']\n","labels = data['label']"],"metadata":{"id":"ohUsfj_zSuY2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from transformers import BertTokenizer, BertModel\n","\n","class LanguageModel(nn.Module):\n","    def __init__(self, num_classes):\n","        super(LanguageModel, self).__init__()\n","        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","        self.bert = BertModel.from_pretrained('bert-base-uncased')\n","        self.dropout = nn.Dropout(0.3)\n","        self.fc1 = nn.Linear(768, num_classes)\n","\n","    def forward(self, input_ids, attention_mask):\n","        _, pooled_output = self.bert(input_ids=input_ids, attention_mask=attention_mask, return_dict=False)\n","        output = self.dropout(pooled_output)\n","        output = self.fc1(output)\n","        return output\n","\n","class CapsuleNetwork(nn.Module):\n","    def __init__(self, num_classes):\n","        super(CapsuleNetwork, self).__init__()\n","        self.language_model = LanguageModel(num_classes)\n","        self.capsule_layer = nn.Sequential(\n","            nn.Linear(num_classes, 32),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(0.3),\n","            nn.Linear(32, 16),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(0.3),\n","            nn.Linear(16, num_classes * 16),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.num_classes = num_classes\n","        self.capsule_activation = nn.Sigmoid()\n","\n","    def forward(self, input_ids, attention_mask):\n","        output = self.language_model(input_ids, attention_mask)\n","        output = self.capsule_layer(output)\n","        output = output.view(-1, self.num_classes, 16)\n","        output = self.capsule_activation(output)\n","        class_capsules = torch.sum(output, dim=2)\n","        class_probabilities = F.softmax(class_capsules, dim=1)\n","        return class_probabilities\n"],"metadata":{"id":"uNCXiATUTQ6W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Tokenize text\n","tokens = tokenizer.batch_encode_plus(\n","    text.tolist(),\n","    max_length=128,\n","    pad_to_max_length=True,\n","    truncation=True,\n","    return_token_type_ids=False\n",")\n","\n","# Extract input IDs and attention masks from tokens\n","input_ids = torch.tensor(tokens['input_ids'])\n","attention_masks = torch.tensor(tokens['attention_mask'])\n","labels = torch.tensor(labels.tolist())\n"],"metadata":{"id":"mHG7lsv9S3AQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import torch\n","import torch.nn as nn\n","from transformers import BertModel, BertTokenizer\n","\n","class CapsuleNetwork(nn.Module):\n","    def __init__(self, num_classes):\n","        super(CapsuleNetwork, self).__init__()\n","        self.num_classes = num_classes\n","        self.bert = BertModel.from_pretrained('bert-base-uncased')\n","        self.capsule = nn.Sequential(\n","            nn.Linear(768, 256),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(256, 64),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(64, num_classes)\n","        )\n","\n","    def forward(self, input_ids, attention_masks):\n","        outputs = self.bert(input_ids, attention_mask=attention_masks)\n","        sequence_output = outputs.last_hidden_state\n","        pooled_output = outputs.pooler_output\n","        capsule_input = pooled_output.view(-1, 1, 768)\n","        capsule_output = self.capsule(capsule_input).squeeze()\n","        return capsule_output\n","\n","\n","\n"],"metadata":{"id":"Cs5xerh7T4qP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load data from CSV file\n","data = pd.read_csv('/content/drive/MyDrive/new/data.csv',nrows=500)\n","text = data['tweet']\n","labels = data['label']\n","\n","# Tokenize text\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","tokens = tokenizer.batch_encode_plus(\n","    text.tolist(),\n","    max_length=128,\n","    pad_to_max_length=True,\n","    truncation=True,\n","    return_token_type_ids=False\n",")\n","input_ids = torch.tensor(tokens['input_ids'])\n","attention_masks = torch.tensor(tokens['attention_mask'])\n","labels = torch.tensor(labels.tolist())\n","\n","# Split data into training and testing sets\n","dataset = torch.utils.data.TensorDataset(input_ids, attention_masks, labels)\n","train_size = int(0.8 * len(dataset))\n","test_size = len(dataset) - train_size\n","train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n"],"metadata":{"id":"AzWumxK3T9v_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_ids"],"metadata":{"id":"Gi_wIJG2XzWQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["attention_masks"],"metadata":{"id":"tg8tGq5lX3mI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels"],"metadata":{"id":"3C5iH6p4X6DI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text"],"metadata":{"id":"zCpZ5njCYGg6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Instantiate the capsule network model\n","model = CapsuleNetwork(num_classes=2)\n","\n","# Define loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n","\n","# Train the model\n","batch_size = 32\n","num_epochs = 10\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"],"metadata":{"id":"_lIqhZZFUsCd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt"],"metadata":{"id":"xJxPuVQ2ro3i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_losses = []\n","test_accuracies = []\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss = 0.0\n","    for batch in train_loader:\n","        input_ids, attention_masks, labels = batch\n","        optimizer.zero_grad()\n","        outputs = model(input_ids, attention_masks)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item() * input_ids.size(0)\n","    epoch_loss = running_loss / len(train_dataset)\n","    train_losses.append(epoch_loss)\n","\n","    model.eval()\n","    num_correct = 0\n","    for batch in test_loader:\n","        input_ids, attention_masks, labels = batch\n","        with torch.no_grad():\n","            outputs = model(input_ids, attention_masks)\n","        preds = torch.argmax(outputs, dim=1)\n","        num_correct += torch.sum(preds == labels)\n","    accuracy = num_correct / len(test_dataset)\n","    test_accuracies.append(accuracy)\n","\n","    print(f'Epoch {epoch+1}/{num_epochs} - Train Loss: {epoch_loss:.4f}, Test Accuracy: {accuracy:.4f}')\n","\n","# Plotting the graph\n","plt.plot(range(1, num_epochs+1), train_losses, label='Train Loss')\n","plt.plot(range(1, num_epochs+1), test_accuracies, label='Test Accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Value')\n","plt.title('Accuracy and Loss')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"yw1No5Hnrk_a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.tensorboard import SummaryWriter\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import precision_recall_curve"],"metadata":{"id":"1KFZV0qKvWu2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device"],"metadata":{"id":"NtGYCN5RwuJZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["training and performance evaluation"],"metadata":{"id":"nj-Z8VZdzPvv"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torch.utils.tensorboard import SummaryWriter\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import precision_recall_curve\n","\n","# Define your model, criterion, optimizer, and data loaders\n","# ...\n","\n","# Set up TensorBoard writer\n","writer = SummaryWriter()\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","train_losses = []\n","test_accuracies = []\n","\n","model = model.to(device)  # Move the model to the appropriate device\n","optimizer = optim.Adam(model.parameters())  # Initialize the optimizer with model parameters\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss = 0.0\n","    for batch in train_loader:\n","        input_ids, attention_masks, labels = batch\n","        optimizer.zero_grad()\n","        input_ids = input_ids.to(device)\n","        attention_masks = attention_masks.to(device)\n","        labels = labels.to(device)\n","        outputs = model(input_ids, attention_masks)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item() * input_ids.size(0)\n","    epoch_loss = running_loss / len(train_dataset)\n","    train_losses.append(epoch_loss)\n","\n","    model.eval()\n","    num_correct = 0\n","    predictions = torch.empty(0, dtype=torch.float32).to(device)\n","    targets = torch.empty(0, dtype=torch.float32).to(device)\n","    for batch in test_loader:\n","        input_ids, attention_masks, labels = batch\n","        input_ids = input_ids.to(device)\n","        attention_masks = attention_masks.to(device)\n","        labels = labels.to(device)\n","        with torch.no_grad():\n","            outputs = model(input_ids, attention_masks)\n","        preds = torch.argmax(outputs, dim=1)\n","        num_correct += torch.sum(preds == labels)\n","\n","        # Append predictions and targets for PR curve calculation\n","        predictions = torch.cat((predictions, outputs[:, 1].cpu().float().to(device)))\n","        targets = torch.cat((targets, labels.float().to(device)))\n","\n","    accuracy = num_correct / len(test_dataset)\n","    test_accuracies.append(accuracy)\n","\n","    print(f'Epoch {epoch+1}/{num_epochs} - Train Loss: {epoch_loss:.4f}, Test Accuracy: {accuracy:.4f}')\n","\n","    # Write accuracy and loss to TensorBoard as scalars\n","    writer.add_scalar('Metrics/Train Loss', epoch_loss, epoch)\n","    writer.add_scalar('Metrics/Test Accuracy', accuracy, epoch)\n","\n","    # Write distribution of model parameters to TensorBoard\n","    for name, param in model.named_parameters():\n","        writer.add_histogram(name, param, epoch)\n","\n","    # Calculate PR curve and write to TensorBoard\n","    precision, recall, _ = precision_recall_curve(targets.cpu(), predictions.cpu())\n","    writer.add_pr_curve('Metrics/PR Curve', torch.tensor(targets.cpu()), torch.tensor(predictions.cpu()), global_step=epoch)\n","\n","# Close TensorBoard writer\n","writer.close()\n"],"metadata":{"id":"8cI1bo-dzCxH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.tensorboard import SummaryWriter\n","\n","# Load the TensorBoard log directory\n","log_dir = \"/content/runs\"\n","\n","# Create a SummaryWriter to visualize the loaded data\n","writer = SummaryWriter(log_dir=log_dir)\n","\n","# Close the SummaryWriter\n","writer.close()\n"],"metadata":{"id":"pd4sJXOW1BGv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%load_ext tensorboard"],"metadata":{"id":"RQHFqdWK1LT1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%tensorboard --logdir runs\n","%reload_ext tensorboard"],"metadata":{"id":"uZCXakog1UNu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%reload_ext tensorboard"],"metadata":{"id":"IspuXHrW1aBA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define a file path to save the model\n","PATH = \"/content/drive/MyDrive/new/capsule_net new.pth\"\n","\n","# Save the model's state dictionary to a file\n","torch.save(model.state_dict(), PATH)"],"metadata":{"id":"T5leV3dCjNsD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Testing"],"metadata":{"id":"1cjNQMrQ9-zy"}},{"cell_type":"code","source":["import torch\n","from transformers import BertTokenizer, RobertaForSequenceClassification\n","\n","# Load the saved capsule-net model\n","model_path = '/content/drive/MyDrive/new/capsule_net new.pth'\n","model_state_dict = torch.load(model_path)\n","model = RobertaForSequenceClassification.from_pretrained('roberta-base', state_dict=model_state_dict)\n","\n","# Tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Get real-time user input\n","while True:\n","    user_input = input('Enter a comment (or type \"exit\" to stop): ')\n","    if user_input == 'exit':\n","        break\n","\n","    # Tokenize the user input\n","    tokenized_input = tokenizer.tokenize(user_input)\n","    input_ids = tokenizer.convert_tokens_to_ids(tokenized_input)\n","    input_ids = torch.tensor([input_ids])\n","    attention_masks = torch.where(input_ids != 0, torch.tensor(1), torch.tensor(0))\n","\n","    # Make prediction\n","    model.eval()\n","    with torch.no_grad():\n","        outputs = model(input_ids, attention_masks)\n","        logits = outputs[0]\n","        predictions = torch.argmax(logits, dim=1)\n","\n","    # Print the prediction\n","    for prediction in predictions:\n","        if prediction == 0:\n","            print('The comment is not toxic.')\n","        else:\n","            print('The comment is toxic.')\n"],"metadata":{"id":"p7DUmlUeItqx"},"execution_count":null,"outputs":[]}]}